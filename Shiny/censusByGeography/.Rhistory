set.seed(seed)
p <- list(Name = name, seed = seed,
depth = sample(1:5, 1),
l1 = runif(1, 0, .01),
l2 = runif(1, 0, .01),
input_dropout = rbeta(1, 1, 12),
rho = runif(1, .9, .999),
epsilon = runif(1, 1e-10, 1e-4))
p$neurons <- sample(20:600, p$depth, TRUE)
p$hidden_dropout <- rbeta(p$depth, 1.5, 1)/2
if (run) {model <- h2o.deeplearning(x = colnames(use.train.x), y = "Outcome", training_frame = h2oactivity.train,
activation = "RectifierWithDropout", hidden = p$neurons, epochs = 100, loss = "CrossEntropy",
input_dropout_ratio = p$input_dropout, hidden_dropout_ratios = p$hidden_dropout,
l1 = p$l1, l2 = p$l2, rho = p$rho, epsilon = p$epsilon, export_weights_and_biases = TRUE, model_id = p$Name)
## performance on training data
p$MSE <- h2o.mse(model)
p$R2 <- h2o.r2(model)
p$Logloss <- h2o.logloss(model)
p$CM <- h2o.confusionMatrix(model)
## performance on testing data
perf <- h2o.performance(model, h2oactivity.test)
p$T.MSE <- h2o.mse(perf)
p$T.R2 <- h2o.r2(perf)
p$T.Logloss <- h2o.logloss(perf)
p$T.CM <- h2o.confusionMatrix(perf)
} else {
model <- NULL
}
return(list(Params = p, Model = model))
}
model.res <- lapply(use.seeds, myRun)
model.res.dat <- do.call(rbind, lapply(model.res, function(x) with(x$Params,
data.frame(l1 = l1, l2 = l2,
depth = depth, input_dropout = input_dropout,
SumNeurons = sum(neurons),
MeanHiddenDropout = mean(hidden_dropout),
rho = rho, epsilon = epsilon, MSE = T.MSE))))
myRun <- function(seed, name = paste0("m_", seed), run = TRUE) {
set.seed(seed)
p <- list(Name = name, seed = seed,
depth = sample(1:5, 1),
l1 = runif(1, 0, .01),
l2 = runif(1, 0, .01),
input_dropout = rbeta(1, 1, 12),
rho = runif(1, .9, .999),
epsilon = runif(1, 1e-10, 1e-4))
p$neurons <- sample(20:200, p$depth, TRUE)
p$hidden_dropout <- rbeta(p$depth, 1.5, 1)/2
if (run) {model <- h2o.deeplearning(x = colnames(use.train.x), y = "Outcome", training_frame = h2oactivity.train,
activation = "RectifierWithDropout", hidden = p$neurons, epochs = 100, loss = "CrossEntropy",
input_dropout_ratio = p$input_dropout, hidden_dropout_ratios = p$hidden_dropout,
l1 = p$l1, l2 = p$l2, rho = p$rho, epsilon = p$epsilon, export_weights_and_biases = TRUE, model_id = p$Name)
## performance on training data
p$MSE <- h2o.mse(model)
p$R2 <- h2o.r2(model)
p$Logloss <- h2o.logloss(model)
p$CM <- h2o.confusionMatrix(model)
## performance on testing data
perf <- h2o.performance(model, h2oactivity.test)
p$T.MSE <- h2o.mse(perf)
p$T.R2 <- h2o.r2(perf)
p$T.Logloss <- h2o.logloss(perf)
p$T.CM <- h2o.confusionMatrix(perf)
} else {
model <- NULL
}
return(list(Params = p, Model = model))
}
use.seeds <- c(403L, 10L, 329737957L, -753102721L, 1148078598L, -1945176688L, -1395587021L, -1662228527L, 367521152L, 217718878L, 1370247081L,
571790939L, -2065569174L, 1584125708L, 1987682639L, 818264581L, 1748945084L, 264331666L, 1408989837L, 2010310855L, 1080941998L)
model.res <- lapply(use.seeds, myRun)
model.res.dat <- do.call(rbind, lapply(model.res, function(x)
with(x$Params,
data.frame(l1 = l1, l2 = l2,
depth = depth,
input_dropout = input_dropout,
SumNeurons = sum(neurons),
MeanHiddenDropout = mean(hidden_dropout),
rho = rho, epsilon = epsilon, MSE = T.MSE))))
p.perf <- ggplot(melt(model.res.dat, id.vars = c("MSE")), aes(value, MSE)) +  geom_point() +  stat_smooth(color = "black") +
facet_wrap(~ variable, scales = "free_x", ncol = 2) +  theme_classic()
print(p.perf)
model.res.dat <- do.call(rbind, lapply(model.res, function(x)
with(x$Params,
data.frame(l1 = l1, l2 = l2,
depth = depth,
input_dropout = input_dropout,
SumNeurons = sum(neurons),
MeanHiddenDropout = mean(hidden_dropout),
rho = rho, epsilon = epsilon, MSE = T.MSE))))
p.perf <- ggplot(melt(model.res.dat, id.vars = c("MSE")), aes(value, MSE)) +  geom_point() +  stat_smooth(color = "black") +
facet_wrap(~ variable, scales = "free_x", ncol = 2) +  theme_classic()
print(p.perf)
m.gam <- gam(MSE ~ s(l1, k = 4) + s(l2, k = 4) + s(input_dropout) +  s(rho, k = 4) + s(epsilon, k = 4) +
s(MeanHiddenDropout, k = 4) + te(depth, SumNeurons, k = 4), data = model.res.dat)
library(mgcv)
m.gam <- gam(MSE ~ s(l1, k = 4) + s(l2, k = 4) + s(input_dropout) +  s(rho, k = 4) + s(epsilon, k = 4) +
s(MeanHiddenDropout, k = 4) + te(depth, SumNeurons, k = 4), data = model.res.dat)
m.gam <- gam(MSE ~ s(l1, k = 2) + s(l2, k = 2) + s(input_dropout) +  s(rho, k = 2) + s(epsilon, k = 2) +
s(MeanHiddenDropout, k = 2) + te(depth, SumNeurons, k = 2), data = model.res.dat)
m.gam <- gam(MSE ~ s(l1, k = 2) + s(l2, k = 4) + s(input_dropout) +  s(rho, k = 4) + s(epsilon, k = 4) +
s(MeanHiddenDropout, k = 4) + te(depth, SumNeurons, k = 4), data = model.res.dat)
use.seeds <- c(403L, 10L, 329737957L, -753102721L, 1148078598L, -1945176688L, -1395587021L, -1662228527L, 367521152L, 217718878L, 1370247081L,
571790939L, -2065569174L, 1584125708L, 1987682639L, 818264581L, 1748945084L, 264331666L, 1408989837L, 2010310855L, 1080941998L, 1107560456L,
-1697965045L, 1540094185L, 1807685560L, 2015326310L, -1685044991L, 1348376467L, -1013192638L, -757809164L, 1815878135L, -1183855123L, -91578748L, -1942404950L, -846262763L, -497569105L, -1489909578L, 1992656608L, -778110429L, -313088703L, -758818768L, -696909234L, 673359545L, 1084007115L, -1140731014L, -877493636L, -1319881025L, 3030933L, -154241108L, -1831664254L)
model.res <- lapply(use.seeds, myRun)
myRun <- function(seed, name = paste0("m_", seed), run = TRUE) {
set.seed(seed)
p <- list(Name = name, seed = seed,
depth = sample(1:5, 1),
l1 = runif(1, 0, .01),
l2 = runif(1, 0, .01),
input_dropout = rbeta(1, 1, 12),
rho = runif(1, .9, .999),
epsilon = runif(1, 1e-10, 1e-4))
p$neurons <- sample(20:200, p$depth, TRUE)
p$hidden_dropout <- rbeta(p$depth, 1.5, 1)/2
if (run) {model <- h2o.deeplearning(x = colnames(use.train.x), y = "Outcome", training_frame = h2oactivity.train,
#activation = "RectifierWithDropout",
activation = "TanhWithDropout",
hidden = p$neurons, epochs = 100, loss = "CrossEntropy",
input_dropout_ratio = p$input_dropout, hidden_dropout_ratios = p$hidden_dropout,
l1 = p$l1, l2 = p$l2, rho = p$rho, epsilon = p$epsilon, export_weights_and_biases = TRUE, model_id = p$Name)
## performance on training data
p$MSE <- h2o.mse(model)
p$R2 <- h2o.r2(model)
p$Logloss <- h2o.logloss(model)
p$CM <- h2o.confusionMatrix(model)
## performance on testing data
perf <- h2o.performance(model, h2oactivity.test)
p$T.MSE <- h2o.mse(perf)
p$T.R2 <- h2o.r2(perf)
p$T.Logloss <- h2o.logloss(perf)
p$T.CM <- h2o.confusionMatrix(perf)
} else {
model <- NULL
}
return(list(Params = p, Model = model))
}
use.seeds <- c(403L, 10L, 329737957L, -753102721L, 1148078598L, -1945176688L, -1395587021L, -1662228527L, 367521152L, 217718878L, 1370247081L,
571790939L, -2065569174L, 1584125708L, 1987682639L, 818264581L, 1748945084L, 264331666L, 1408989837L, 2010310855L, 1080941998L, 1107560456L,
-1697965045L, 1540094185L, 1807685560L, 2015326310L, -1685044991L, 1348376467L, -1013192638L, -757809164L, 1815878135L, -1183855123L, -91578748L, -1942404950L, -846262763L, -497569105L, -1489909578L, 1992656608L, -778110429L, -313088703L, -758818768L, -696909234L, 673359545L, 1084007115L)
use.seeds <- c(1234, 2345, 3456, 4567, 5678, 6789, 78900, 1122, 2233, 3344, 4455, 5566, 6677, 7788, 8899, 9988, 8877, 7766, 6655, 5544, 4433, 3322, 2211)
model.res <- lapply(use.seeds, myRun)
model.res.dat <- do.call(rbind, lapply(model.res, function(x)
with(x$Params,
data.frame(l1 = l1, l2 = l2,
depth = depth,
input_dropout = input_dropout,
SumNeurons = sum(neurons),
MeanHiddenDropout = mean(hidden_dropout),
rho = rho, epsilon = epsilon, MSE = T.MSE))))
p.perf <- ggplot(melt(model.res.dat, id.vars = c("MSE")), aes(value, MSE)) +  geom_point() +  stat_smooth(color = "black") +
facet_wrap(~ variable, scales = "free_x", ncol = 2) +  theme_classic()
print(p.perf)
m.gam <- gam(MSE ~ s(l1, k = 2) + s(l2, k = 4) + s(input_dropout) +  s(rho, k = 4) + s(epsilon, k = 4) +
s(MeanHiddenDropout, k = 4) + te(depth, SumNeurons, k = 4), data = model.res.dat)
m.gam <- gam(MSE ~ s(l1, k = 4) + s(l2, k = 4) + s(input_dropout) +  s(rho, k = 4) + s(epsilon, k = 4) +
s(MeanHiddenDropout, k = 4) + te(depth, SumNeurons, k = 4), data = model.res.dat)
saveRDS(model.res, "../Essentials/modelres.rds")
try(h2o.shutdown(), silent = TRUE)
try(rm(list=ls()), silent = TRUE)
try(lapply(paste('package:',names(sessionInfo()$otherPkgs),sep=""), detach,character.only=TRUE,unload=TRUE), silent = TRUE)
#Apply packages
if(!require(easypackages)){
install.packages("easypackages")
library(easypackages)
}
packages("caret", "h2o", prompt = FALSE)
use.train.x <- read.table("../Essentials/data/X_train.txt")
use.train.y <- read.table("../Essentials/data/y_train.txt")[[1]]
use.test.x <- read.table("../Essentials/data/X_test.txt")
use.test.y <- read.table("../Essentials/data/y_test.txt")[[1]]
use.labels <- read.table("../Essentials/data/activity_labels.txt")
localH2O = h2o.init(max_mem_size = "3G", nthreads = 2)
h2oactivity.train <- as.h2o(use.train.x, destination_frame = "h2oactivitytrain")
h2oactivity.test <- as.h2o(use.test.x, destination_frame = "h2oactivitytest")
mu1 <- h2o.deeplearning(x = colnames(h2oactivity.train), training_frame= h2oactivity.train, validation_frame = h2oactivity.test,
activation = "Tanh", autoencoder = TRUE, hidden = c(100, 100), epochs = 30, sparsity_beta = 0,
input_dropout_ratio = 0, l1 = 0, l2 = 0)
save(mu1, file="../Essentials/mu1-2.RData")
load("..//Essentials/mu1-2.RData")
mu1
erroru1 <- as.data.frame(h2o.anomaly(mu1-2, h2oactivity.train))
erroru1 <- as.data.frame(h2o.anomaly(mu1, h2oactivity.train))
save(erroru1, file="../Essentials/erroru1.RData")
pue1 <- ggplot(erroru1, aes(Reconstruction.MSE)) +  geom_histogram(binwidth = .001, fill = "grey50") +
geom_vline(xintercept = quantile(erroru1[[1]], probs = .99), linetype = 2) + theme_bw()
print(pue1)
i.anomolous <- erroru1$Reconstruction.MSE >= quantile(erroru1[[1]], probs = .99)
use.train.x <- read.table("../Essentials/data/X_train.txt")
use.train.y <- read.table("../Essentials/data/y_train.txt")[[1]]
use.test.x <- read.table("../Essentials/data/X_test.txt")
use.test.y <- read.table("../Essentials/data/y_test.txt")[[1]]
use.labels <- read.table("../Essentials/data/activity_labels.txt")
localH2O = h2o.init(max_mem_size = "3G", nthreads = 2)
h2oactivity.train <- as.h2o(use.train.x, destination_frame = "h2oactivitytrain")
h2oactivity.test <- as.h2o(use.test.x, destination_frame = "h2oactivitytest")
save(use.test.x, use.test.y, use.train.x, use.train.y, h2odigits.train, h2odigits.test, use.labels, file="../Essentials/getDataWalkthru2.RData")
save(use.test.x, use.test.y, use.train.x, use.train.y, h2oactivity.train, h2oactivity.test, use.labels, file="../Essentials/getDataWalkthru2.RData")
try(h2o.shutdown(prompt = FALSE), silent = TRUE)
rm(list=ls())
#Remove all packages loaded in environment
try(lapply(paste('package:', names(sessionInfo()$otherPkgs), sep=""), detach, character.only=TRUE, unload=TRUE), silent = TRUE)
#Apply packages
if(!require(easypackages)){
install.packages("easypackages")
library(easypackages)
}
packages("data.table", "ggplot2", "parallel", "caret", "h2o", prompt = FALSE)
# data and H2O setup
digits.train <- read.csv("../Essentials/data/train.csv")
digits.train$label <- factor(digits.train$label, levels = 0:9)
cl <- makeCluster(2)
cl <- h2o.init(max_mem_size = "6G", nthreads = 4)
h2odigits <- as.h2o(digits.train, destination_frame = "h2odigits")
i <- 1:20000
h2odigits.train <- h2odigits[i, -1]
itest <- 20001:30000
h2odigits.test <- h2odigits[itest, -1]
xnames <- colnames(h2odigits.train)
## create 5 folds
set.seed(1234)
folds <- createFolds(1:20000, k = 5)
if(!require(easypackages)){
install.packages("easypackages")
library(easypackages)
}
packages("caret", "nnet", prompt = FALSE)
rm(list=ls())
#Remove all packages loaded in environment
lapply(paste('package:', names(sessionInfo()$otherPkgs), sep=""), detach, character.only=TRUE, unload=TRUE)
#Apply packages
if(!require(easypackages)){
install.packages("easypackages")
library(easypackages)
}
packages("data.table", "ggplot2", "parallel", "caret", "h2o", prompt = FALSE)
# data and H2O setup
use.train.x <- read.table("../Essentials/data/X_train.txt")
use.train.y <- read.table("../Essentials/data/y_train.txt")[[1]]
use.test.x <- read.table("../Essentials/data/X_test.txt")
use.test.y <- read.table("../Essentials/data/y_test.txt")[[1]]
use.train <- cbind(use.train.x, Outcome = factor(use.train.y))
use.test <- cbind(use.test.x, Outcome = factor(use.test.y))
use.labels <- read.table("../Essentials/data/activity_labels.txt")
localH2O = h2o.init(max_mem_size = "3G", nthreads = 2)
h2oactivity.train <- as.h2o(use.train, destination_frame = "h2oactivitytrain")
h2oactivity.test <- as.h2o(use.test, destination_frame = "h2oactivitytest")
save(use.train.x, use.train.y, use.test.x, use.test.y, use.labels, h2oactivity.train, h2oactivity.test, file="../Essentials.RData")
save(use.train.x, use.train.y, use.test.x, use.test.y, use.labels, h2oactivity.train, h2oactivity.test, file="../Essentials/Part5Data.RData")
mt1 <- h2o.deeplearning(  x = colnames(use.train.x),  y = "Outcome",  training_frame= h2oactivity.train,
activation = "RectifierWithDropout",  hidden = c(50),  epochs = 10,  loss = "CrossEntropy",
input_dropout_ratio = .2,  hidden_dropout_ratios = c(.5), ,  export_weights_and_biases = TRUE )
localH2O = h2o.init(max_mem_size = "3G", nthreads = 2)
load("../Essentials/Part5Data.RData")
mt1 <- h2o.deeplearning(x = colnames(use.train.x), y = "Outcome",  training_frame= h2oactivity.train,
activation = "RectifierWithDropout",  hidden = c(50),  epochs = 10,  loss = "CrossEntropy",
input_dropout_ratio = .2, hidden_dropout_ratios = c(.5), , export_weights_and_biases = TRUE )
mt1
f <- as.data.frame(h2o.deepfeatures(mt1, h2oactivity.train, 1))
f[1:10, 1:5]
w1 <- as.matrix(h2o.weights(mt1, 1))
## plot heatmap of the weights
tmp <- as.data.frame(t(w1))
tmp$Row <- 1:nrow(tmp)
tmp <- melt(tmp, id.vars = c("Row"))
p.heat <- ggplot(tmp, aes(variable, Row, fill = value)) +  geom_tile() +  scale_fill_gradientn(colours = c("black", "white", "blue")) +
theme_classic() +  theme(axis.text = element_blank()) +  xlab("Hidden Neuron") +  ylab("Input Variable") +
ggtitle("Heatmap of Weights for Layer 1")
print(p.heat)
yhat.h2o <- as.data.frame(h2o.predict(mt1, newdata = h2oactivity.train))
head(yhat.h2o)
mt1 <- h2o.deeplearning(x = colnames(use.train.x), y = "Outcome",  training_frame= h2oactivity.train,
activation = "RectifierWithDropout",  hidden = c(50),  epochs = 10,  loss = "CrossEntropy",
input_dropout_ratio = .2, hidden_dropout_ratios = c(.5), , export_weights_and_biases = TRUE,
h2o.no_progress(hidden))
mt1 <- h2o.deeplearning(x = colnames(use.train.x), y = "Outcome",  training_frame= h2oactivity.train,
activation = "RectifierWithDropout",  hidden = c(50),  epochs = 10,  loss = "CrossEntropy",
input_dropout_ratio = .2, hidden_dropout_ratios = c(.5), , export_weights_and_biases = TRUE, h2o.no_progress())
rm(list=ls())
#Remove all packages loaded in environment
try(lapply(paste('package:', names(sessionInfo()$otherPkgs), sep=""), detach, character.only=TRUE, unload=TRUE), silent = TRUE)
if(!require(easypackages)){
install.packages("easypackages")
library(easypackages)
}
packages("data.table", "ggplot2", "parallel", "caret", "h2o", prompt = FALSE)
localH2O = h2o.init(max_mem_size = "3G", nthreads = 2)
h20.shutdown()
use.train.x <- read.table("../Essentials/data/X_train.txt")
use.train.y <- read.table("../Essentials/data/y_train.txt")[[1]]
use.test.x <- read.table("../Essentials/data/X_test.txt")
use.test.y <- read.table("../Essentials/data/y_test.txt")[[1]]
use.train <- cbind(use.train.x, Outcome = factor(use.train.y))
use.test <- cbind(use.test.x, Outcome = factor(use.test.y))
use.labels <- read.table("../Essentials/data/activity_labels.txt")
save(use.train.x, use.train.y, use.test.x, use.test.y, use.labels,  use.train, file="../Essentials/Part5Data.RData")
use.train.x <- read.table("../Essentials/data/X_train.txt")
use.train.y <- read.table("../Essentials/data/y_train.txt")[[1]]
use.test.x <- read.table("../Essentials/data/X_test.txt")
use.test.y <- read.table("../Essentials/data/y_test.txt")[[1]]
use.train <- cbind(use.train.x, Outcome = factor(use.train.y))
use.test <- cbind(use.test.x, Outcome = factor(use.test.y))
use.labels <- read.table("../Essentials/data/activity_labels.txt")
save(use.train.x, use.train.y, use.test.x, use.test.y, use.labels, use.train, use.test, file="../Essentials/Part5Data.RData")
colnames(use.train.x)
h2oactivity.train
mt1 <- h2o.deeplearning(x = colnames(use.train.x), y = "Outcome",  training_frame= h2oactivity.train,
activation = "RectifierWithDropout",  hidden = c(50),  epochs = 10,  loss = "CrossEntropy",
input_dropout_ratio = .2, hidden_dropout_ratios = c(.5), , export_weights_and_biases = TRUE)
h2oactivity.train <- as.h2o(use.train, destination_frame = "h2oactivitytrain")
h2oactivity.train <- as.h2o(use.train, destination_frame = "h2oactivitytrain", h2o.no_progress())
h2oactivity.test <- as.h2o(use.test, destination_frame = "h2oactivitytest", h2o.no_progress())
mt1 <- h2o.deeplearning(x = colnames(use.train.x), y = "Outcome",  training_frame= h2oactivity.train,
activation = "RectifierWithDropout",  hidden = c(50),  epochs = 10,  loss = "CrossEntropy",
input_dropout_ratio = .2, hidden_dropout_ratios = c(.5), , export_weights_and_biases = TRUE)
mt1 <- h2o.deeplearning(x = colnames(use.train.x), y = "Outcome",  training_frame= h2oactivity.train,
activation = "RectifierWithDropout",  hidden = c(50),  epochs = 10,  loss = "CrossEntropy",
input_dropout_ratio = .2, hidden_dropout_ratios = c(.5), , export_weights_and_biases = TRUE,
h2o.no_progress())
mt1 <- h2o.deeplearning(
x = colnames(use.train.x),
y = "Outcome",
training_frame = h2oactivity.train,
activation = "RectifierWithDropout",
hidden = c(50),
epochs = 10,
rate = .005,
loss = "CrossEntropy",
input_dropout_ratio = .2,
hidden_dropout_ratios = c(.5),
export_weights_and_biases = TRUE
)
load("../Essentials/Part5Data.RData")
localH2O = h2o.init(max_mem_size = "3G", nthreads = 2)
rm(list=ls())
#Remove all packages loaded in environment
try(lapply(paste('package:', names(sessionInfo()$otherPkgs), sep=""), detach, character.only=TRUE, unload=TRUE), silent = TRUE)
if(!require(easypackages)){
install.packages("easypackages")
library(easypackages)
}
packages("data.table", "ggplot2", "parallel", "caret", "h2o", prompt = FALSE)
localH2O = h2o.init(max_mem_size = "3G", nthreads = 2)
h2oactivity.train <- as.h2o(use.train, destination_frame = "h2oactivitytrain", h2o.no_progress())
load("../Essentials/Part5Data.RData")
h2oactivity.train <- as.h2o(use.train, destination_frame = "h2oactivitytrain", h2o.no_progress())
h2oactivity.test <- as.h2o(use.test, destination_frame = "h2oactivitytest", h2o.no_progress())
mt1 <- h2o.deeplearning(x = colnames(use.train.x), y = "Outcome",  training_frame= h2oactivity.train,
activation = "RectifierWithDropout",  hidden = c(50),  epochs = 10,  loss = "CrossEntropy",
input_dropout_ratio = .2, hidden_dropout_ratios = c(.5), , export_weights_and_biases = TRUE,
h2o.no_progress())
mt1 <- h2o.deeplearning(
x = colnames(use.train.x),
y = "Outcome",
training_frame = h2oactivity.train,
activation = "RectifierWithDropout",
hidden = c(50),
epochs = 10,
rate = .005,
loss = "CrossEntropy",
input_dropout_ratio = .2,
hidden_dropout_ratios = c(.5),
export_weights_and_biases = TRUE
)
mt1 <- h2o.deeplearning(x = colnames(use.train.x), y = "Outcome", training_frame = h2oactivity.train,
activation = "RectifierWithDropout", hidden = c(50), epochs = 10, rate = .005, loss = "CrossEntropy",
input_dropout_ratio = .2, hidden_dropout_ratios = c(.5), export_weights_and_biases = TRUE
)
mt1
f <- as.data.frame(h2o.deepfeatures(mt1, h2oactivity.train, 1))
f[1:10, 1:5]
w1 <- as.matrix(h2o.weights(mt1, 1))
## plot heatmap of the weights
tmp <- as.data.frame(t(w1))
tmp$Row <- 1:nrow(tmp)
tmp <- melt(tmp, id.vars = c("Row"))
p.heat <- ggplot(tmp, aes(variable, Row, fill = value)) +  geom_tile() +  scale_fill_gradientn(colours = c("black", "white", "blue")) +
theme_classic() +  theme(axis.text = element_blank()) +  xlab("Hidden Neuron") +  ylab("Input Variable") +
ggtitle("Heatmap of Weights for Layer 1")
print(p.heat)
rm(list=ls())
#Remove all packages loaded in environment
try(lapply(paste('package:', names(sessionInfo()$otherPkgs), sep=""), detach, character.only=TRUE, unload=TRUE), silent = TRUE)
#Apply packages
if(!require(easypackages)){
install.packages("easypackages")
library(easypackages)
}
packages("data.table", "ggplot2", "parallel", "caret", "h2o", prompt = FALSE)
# download.file("http://archive.ics.uci.edu/ml/machine-learning-databases/00203/YearPredictionMSD.txt.zip",
#               destfile = "../Essentials/data/YearPredictionMSD.txt.zip")
# unzip("../Essentials/data/YearPredictionMSD.txt.zip")
## read data into R using fread() from data.table package
d <- fread("../Essentials/data/YearPredictionMSD.txt", sep = ",")
p.hist <- ggplot(d[, .(V1)], aes(V1)) +  geom_histogram(binwidth = 1) +  theme_classic() +  xlab("Year of Release")
print(p.hist)
quantile(d$V1, probs = c(.005, .995))
d.train <- d[1:463715][V1 >= 1957 & V1 <= 2010]
d.test <- d[463716:515345][V1 >= 1957 & V1 <= 2010]
localH2O = h2o.init(max_mem_size = "4G", nthreads = 2)
h2omsd.train <- as.h2o(d.train, destination_frame = "h2omsdtrain")
h2omsd.test <- as.h2o(d.test, destination_frame = "h2omsdtest")
summary(m0 <- lm(V1 ~ ., data = d.train))$r.squared
cor(d.test$V1,  predict(m0, newdata = d.test))^2
system.time(m1 <- h2o.deeplearning(x = colnames(d)[-1],  y = "V1",  training_frame= h2omsd.train,  validation_frame = h2omsd.test,
activation = "RectifierWithDropout",  hidden = c(50),  epochs = 100,  input_dropout_ratio = 0,  hidden_dropout_ratios = c(0),
score_training_samples = 0,  score_validation_samples = 0,  diagnostics = TRUE,  export_weights_and_biases = TRUE,
variable_importances = TRUE)
)
m1
system.time(
m2 <- h2o.deeplearning(x = colnames(d)[-1],  y = "V1",  training_frame= h2omsd.train,  validation_frame = h2omsd.test,
activation = "RectifierWithDropout",  hidden = c(200, 300, 400),  epochs = 100,
input_dropout_ratio = 0,  hidden_dropout_ratios = c(.2, .2, .2),  score_training_samples = 0,
score_validation_samples = 0,  diagnostics = TRUE,  export_weights_and_biases = TRUE,
variable_importances = TRUE)
)
mu3 <- c(4.5, 4.5)
Sigma2 <- matrix(c(2.25,1.5,1.5,2.25),nrow=2)
unimodal <- mvrnorm(50, mu=mu3, Sigma=Sigma2)
if(!require(easypackages)){
install.packages("easypackages")
library(easypackages)
}
packages("MASS", "rgl", "boot", prompt = FALSE)
x1 <- c(1, 2, 3, 4, 7, 8, 10)
x2 <- c(8, 2, 3, 1, 11, 8, 10)
X <- cbind(x1, x2)
plot(X, pch=16)
identify(X, labels=1:7)
X.km <- kmeans(X,2)
X.km
points(X,pch=X.km$cluster+1, col=X.km$cluster+1)
points(X.km$centers, col=2:3, pch=2:3, cex=1.5)
mu3 <- c(4.5, 4.5)
Sigma2 <- matrix(c(2.25,1.5,1.5,2.25),nrow=2)
unimodal <- mvrnorm(50, mu=mu3, Sigma=Sigma2)
plot(unimodal, pch=16)
uni.3m1 <- kmeans(unimodal,3)# Random starts -
uni.3m2 <- kmeans(unimodal,3)# different answers
plot(unimodal, pch=uni.3m1$cluster, col=uni.3m1$cluster)
points(unimodal, pch=uni.3m2$cluster+3, col=uni.3m2$cluster)
table(data.frame(km1= uni.3m1$cluster, km2 = uni.3m2$cluster))
km2
km2 = uni.3m2$cluster
km2
table(data.frame(km1 = uni.3m1$cluster, km2 = uni.3m2$cluster))
uni.3m1$withinss
sum(uni.3m1$withinss)
uni.3m2$withinss
sum(uni.3m2$withinss)
uni.3m3 <- kmeans(unimodal,3, nstart=25)
uni.3m4 <- kmeans(unimodal,3, nstart=25)
plot(unimodal, pch=uni.3m3$cluster, col=uni.3m3$cluster)
points(unimodal, pch=uni.3m4$cluster+3, col=uni.3m4$cluster)
table(data.frame(km1=uni.3m3$cluster,km2=uni.3m4$cluster))
uni.3m3$withinss
sum(uni.3m3$withinss)
swiss.4m <- kmeans(scale(swiss), 4, nstart=25)
pairs(swiss, pch=swiss.4m$cluster, col=swiss.4m$cluster)
plot3d(swiss.pca$x[,1:3],type="s",size=.25, col=swiss.4m$cluster)
??swiss.pca
data(swiss)
swiss.4m <- kmeans(scale(swiss), 4, nstart=25)
pairs(swiss, pch=swiss.4m$cluster, col=swiss.4m$cluster)
plot3d(swiss.pca$x[,1:3],type="s",size=.25, col=swiss.4m$cluster)
pseudoF = function(X, k, ns = 25){
nk <- length(k)
n <- nrow(X)
T <- sum(scale(X,scale=F)^2)
W <- rep(T, nk)
for (i in 1:nk)
{cli <- kmeans(X, k[i], nstart=ns)
W[i] <- sum(cli$withinss)
}
B <- T-W
pF <- (B/(k-1))/(W/(n-k))
return(list(k=k, W=W, pF=pF))
}
pseudoF(scale(swiss), 2:6)$k
$W
swiss.4m$W
swiss.4m$pF
swiss.3m <- kmeans(scale(swiss), 3)
sum(swiss.3m$withinss)
pairs(swiss, pch=swiss.3m$cluster, col=swiss.3m$cluster)
plot3d(swiss.pca$x[,1:3],type="s",size=.25, col=swiss.3m$cluster)
swiss.pca = prcomp(swiss, scale=T)
plot3d(swiss.pca$x[,1:3],type="s",size=.25, col=swiss.4m$cluster)
pseudoF(scale(swiss), 2:6)$k
pseudoF(scale(swiss), 2:6)
swiss.hc.w = hclust(dist(scale(swiss)), "ward")
pairs(swiss, pch=swiss.3m$cluster, col=swiss.3m$cluster)
plot3d(swiss.pca$x[,1:3],type="s",size=.25, col=swiss.3m$cluster)
scatter3d(swiss.pca$x[,1],swiss.pca$x[,2],swiss.pca$x[,3],surface=F,
sphere.size=swiss.3m$cluster,group=as.factor(cutree(swiss.hc.w,3)))
library(car)
scatter3d(swiss.pca$x[,1],swiss.pca$x[,2],swiss.pca$x[,3],surface=F,
sphere.size=swiss.3m$cluster,group=as.factor(cutree(swiss.hc.w,3)))
identify3d(swiss.pca$x[,1],swiss.pca$x[,2],swiss.pca$x[,3],labels=row.names(swiss))
install.packages("Rcmdr")
library(Rcmdr)
scatter3d(swiss.pca$x[,1],swiss.pca$x[,2],swiss.pca$x[,3],surface=F,point.col=swiss.4m$cluster) identify3d(swiss.pca$x[,1],swiss.pca$x[,2],swiss.pca$x[,3],labels=row.names(swiss))
scatter3d(swiss.pca$x[,1],swiss.pca$x[,2],swiss.pca$x[,3],surface=F,point.col=swiss.4m$cluster)
dataset = read.csv('./data/Churn_Modelling.csv')
dataset = read.csv('../data/Churn_Modelling.csv')
shiny::runApp('Shiny/censusByGeography')
getwd()
setwd("~/GitHub/Shiny")
runApp('censusByGeography')
